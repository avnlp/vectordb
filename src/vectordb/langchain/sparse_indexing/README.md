# Sparse Indexing (LangChain)

Sparse indexing provides keyword-based search using sparse vector representations rather than dense embeddings. Instead of capturing overall semantic meaning, sparse vectors encode term-level importance weights, enabling precise keyword matching with learned term expansion. This approach excels at finding documents that contain specific terms or phrases, making it complementary to dense semantic search.

The module supports two sparse representation strategies depending on the database. Pinecone, Milvus, and Qdrant use SPLADE or BM25-based sparse embeddings generated by a sparse encoder, which learns to expand and weight vocabulary terms beyond simple term frequency. Weaviate instead uses its built-in BM25 scoring engine, which computes keyword relevance at query time without requiring external sparse embeddings during indexing. Chroma does not support sparse vector storage.

## Overview

- Keyword-aware search using SPLADE sparse embeddings or native BM25
- Learned term expansion via SPLADE for better recall than traditional keyword matching
- Optional RAG answer generation using an LLM (Groq or OpenAI-compatible endpoints)
- Configuration-driven through YAML files with environment variable substitution
- 25 pre-built configuration files covering all database and dataset combinations
- Batch processing support for efficient large-scale indexing

## How It Works

### Indexing

The indexing pipeline loads a dataset and processes each document through a sparse embedding model. For Pinecone, Milvus, and Qdrant, the pipeline generates sparse embeddings that produce a sparse vector of weighted vocabulary indices for each document. These sparse vectors are stored in the database's native sparse format. For Weaviate, the pipeline stores the document text and metadata, because Weaviate computes BM25 scores internally at query time. Chroma does not support sparse vector storage, so the Chroma pipeline is a stub.

### Search

The search pipeline embeds the query text using the same sparse representation strategy used during indexing. For SPLADE-based databases, the query is passed through the sparse embedder to produce a sparse query vector, and the database performs a sparse vector similarity search. For Weaviate, the pipeline issues a native BM25 query that matches against the stored document text. Results are returned ranked by their sparse relevance score.

## Supported Databases

| Database | Status | Sparse Strategy | Notes |
|----------|--------|-----------------|-------|
| Pinecone | Supported | SPLADE/BM25 embeddings | Stored in sparse_values format |
| Milvus | Supported | SPLADE/BM25 embeddings | Uses sparse inverted index |
| Qdrant | Supported | SPLADE/BM25 embeddings | Uses named sparse vector fields |
| Weaviate | Supported | Native BM25 | No external sparse embeddings; computes BM25 at query time |
| Chroma | Not supported | N/A | Chroma does not support sparse vectors |

## Configuration

Each pipeline is driven by a YAML configuration file using flat naming. Below is an example showing the key sections:

```yaml
dataloader:
  type: "triviaqa"           # triviaqa, arc, popqa, factscore, earnings_calls
  split: "test"
  limit: 100
  use_text_splitter: false

embeddings:
  model: "sentence-transformers/all-MiniLM-L6-v2"
  device: "cpu"
  batch_size: 32

sparse_embeddings:
  model: "bm25"
  device: "cpu"

pinecone:                     # Database-specific section (one per config)
  api_key: "${PINECONE_API_KEY}"
  index_name: "lc-sparse-indexing-triviaqa"
  namespace: ""
  dimension: 384
  metric: "cosine"
  recreate: false

search:
  top_k: 10

rag:
  enabled: false
  model: "llama-3.3-70b-versatile"
  api_key: "${GROQ_API_KEY}"
  temperature: 0.7
  max_tokens: 2048

logging:
  name: "lc_sparse_indexing_pinecone"
  level: "INFO"
```

## Directory Structure

```
sparse_indexing/
├── __init__.py
├── README.md
├── indexing/
│   ├── __init__.py
│   ├── pinecone.py                    # Pinecone sparse indexing
│   ├── milvus.py                      # Milvus sparse indexing
│   ├── qdrant.py                      # Qdrant sparse indexing
│   ├── weaviate.py                    # Weaviate BM25 indexing (text-only storage)
│   └── chroma.py                      # Chroma stub (sparse not supported)
├── search/
│   ├── __init__.py
│   ├── pinecone.py                    # Pinecone sparse search
│   ├── milvus.py                      # Milvus sparse search
│   ├── qdrant.py                      # Qdrant sparse search
│   ├── weaviate.py                    # Weaviate native BM25 search
│   └── chroma.py                      # Chroma stub (sparse not supported)
└── configs/                           # 25 YAML configs (5 databases x 5 datasets)
    ├── pinecone_triviaqa.yaml
    ├── pinecone_arc.yaml
    ├── pinecone_popqa.yaml
    ├── pinecone_factscore.yaml
    ├── pinecone_earnings_calls.yaml
    ├── weaviate_triviaqa.yaml
    ├── ...
    ├── chroma_*.yaml
    ├── milvus_*.yaml
    └── qdrant_*.yaml
```

## Related Modules

- `src/vectordb/langchain/semantic_search/` - Dense-only semantic search pipelines
- `src/vectordb/langchain/hybrid_indexing/` - Combined dense and sparse search pipelines
- `src/vectordb/langchain/utils/` - Shared utilities for configuration, embedding, and data loading
- `src/vectordb/dataloaders/` - Dataset loaders for TriviaQA, ARC, PopQA, FactScore, and EarningsCalls
