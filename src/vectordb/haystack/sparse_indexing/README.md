# Sparse Indexing

Sparse indexing provides keyword-based search using sparse vector representations rather than dense embeddings. Instead of capturing overall semantic meaning, sparse vectors encode term-level importance weights, enabling precise keyword matching with learned term expansion. This approach excels at finding documents that contain specific terms or phrases, making it complementary to dense semantic search.

The module supports two sparse representation strategies depending on the database. Pinecone, Milvus, and Qdrant use SPLADE embeddings generated by a sentence-transformers sparse encoder, which learns to expand and weight vocabulary terms beyond simple term frequency. Weaviate instead uses its built-in BM25 scoring engine, which computes keyword relevance at query time without requiring any external sparse embeddings during indexing.

## Overview

- Keyword-aware search using SPLADE sparse embeddings or native BM25
- Learned term expansion via SPLADE for better recall than traditional keyword matching
- Native Haystack sparse embedding components with no FastEmbed dependency
- Configuration-driven through YAML files with environment variable substitution
- 25 pre-built configuration files covering all database and dataset combinations
- Batch processing support for efficient large-scale indexing

## How It Works

### Indexing Phase

The indexing pipeline loads a dataset and processes each document through a sparse embedding model. For Pinecone, Milvus, and Qdrant, the pipeline generates SPLADE embeddings that produce a sparse vector of weighted vocabulary indices for each document. These sparse vectors are stored in the database's native sparse format. For Weaviate, the pipeline stores only the document text and metadata, because Weaviate computes BM25 scores internally at query time. The Chroma pipeline raises an error, as the open-source version of Chroma does not support sparse vector storage.

### Search Phase

The search pipeline embeds the query text using the same sparse representation strategy used during indexing. For SPLADE-based databases, the query is passed through the sparse text embedder to produce a sparse query vector, and the database performs a sparse vector similarity search. For Weaviate, the pipeline issues a native BM25 query that matches against the stored document text. Results are returned ranked by their sparse relevance score.

## Supported Databases

| Database | Status | Sparse Strategy | Notes |
|----------|--------|-----------------|-------|
| Pinecone | Supported | SPLADE embeddings | Stored in sparse_values format |
| Milvus | Supported | SPLADE embeddings | Uses sparse inverted index |
| Qdrant | Supported | SPLADE embeddings | Uses named sparse vector fields |
| Weaviate | Supported | Native BM25 | No external sparse embeddings; computes BM25 at query time |
| Chroma | Not supported | N/A | Open-source Chroma does not support sparse vectors |

## Configuration

Each pipeline is driven by a YAML configuration file. Below is an example showing the key sections:

```yaml
pinecone:                  # Database-specific section (one per config)
  api_key: "${PINECONE_API_KEY}"
  index_name: "sparse-bm25-index"
  namespace: "keywords"

sparse:
  model: "splade-v2"       # Resolves to prithivida/Splade_PP_en_v2
  backend: "torch"
  device: null

dataloader:
  type: "triviaqa"         # triviaqa, arc, popqa, factscore, earnings_calls
  limit: null
  batch_size: 32

indexing:
  batch_size: 100
  show_progress: true

query:
  top_k: 20

logging:
  level: "INFO"
  name: "sparse_indexing"
```

Supported SPLADE model aliases:
- `splade` and `splade-v2` resolve to `prithivida/Splade_PP_en_v2` (recommended)
- `splade-v1` resolves to `prithivida/Splade_PP_en_v1`
- `splade-cocondenser` resolves to `naver/splade-cocondenser-ensembledistil`

## Directory Structure

```
sparse_indexing/
├── __init__.py                        # Package exports for all pipelines
├── README.md
├── indexing/                          # Database-specific indexing pipelines
│   ├── __init__.py
│   ├── pinecone.py                    # Pinecone SPLADE indexing
│   ├── milvus.py                      # Milvus SPLADE indexing
│   ├── qdrant.py                      # Qdrant SPLADE indexing
│   ├── weaviate.py                    # Weaviate BM25 indexing (text-only storage)
│   └── chroma.py                      # Chroma stub (raises NotImplementedError)
├── search/                            # Database-specific search pipelines
│   ├── __init__.py
│   ├── pinecone.py                    # Pinecone sparse search
│   ├── milvus.py                      # Milvus sparse search
│   ├── qdrant.py                      # Qdrant sparse search
│   ├── weaviate.py                    # Weaviate native BM25 search
│   └── chroma.py                      # Chroma stub (raises NotImplementedError)
└── configs/                           # 25 YAML configs (5 databases x 5 datasets)
    ├── pinecone/
    │   ├── triviaqa.yaml
    │   ├── arc.yaml
    │   ├── popqa.yaml
    │   ├── factscore.yaml
    │   └── earnings_calls.yaml
    ├── milvus/                         # Same 5 dataset files per database
    ├── qdrant/
    ├── weaviate/
    └── chroma/
```

## Related Modules

- `src/vectordb/haystack/semantic_search/` - Dense-only semantic search pipelines
- `src/vectordb/haystack/hybrid_indexing/` - Combined dense and sparse search pipelines
- `src/vectordb/haystack/utils/` - Shared utilities for configuration, embedding, and data loading
- `src/vectordb/dataloaders/` - Dataset loaders for TriviaQA, ARC, PopQA, FactScore, and EarningsCalls
